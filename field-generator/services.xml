<?xml version="1.0" encoding="utf-8" ?>
<!-- Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root. -->
<services version="1.0">

    <container id="container" version="1.0">
        
        <!-- Local language model -->
        <component id="llm" class="ai.vespa.llm.clients.LocalLLM">
            <config name="ai.vespa.llm.clients.llm-local-client">
                <!-- Specify LLM by id for Vespa Cloud -->
                <model model-id="phi-3.5-mini-q4"/>-->
                <!-- Alternative is to use url, which also works outside Vespa Cloud -->
<!--            <model url="https://huggingface.co/bartowski/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct-Q4_K_M.gguf"/>-->
                <!-- Number of tokens an LLM can attend in each inference for all parallel request. -->
                <contextSize>5000</contextSize>
                <!-- Requests are processed in parallel using continuous batching.
                Each request will use 5000 / 5 = 1000 context tokens. -->
                <parallelRequests>5</parallelRequests>
                <!--Request context size split between prompt and completion tokens: 500 + 500 = 10000 -->
                <maxPromptTokens>500</maxPromptTokens>
                <maxTokens>500</maxTokens>
                <!-- Documents will be set in a queue to wait until one of the parallel requests is done process. -->
                <maxQueueSize>3</maxQueueSize>
                <!-- Both enqueue and queue wait has to be set proportional to max queues size 
                because the last request will need to wait for all previous ones before starting the processing. -->
                <maxEnqueueWait>100000</maxEnqueueWait>
                <maxQueueWait>100000</maxQueueWait>
                <!-- Context overflow leads to hallucinations, better to skip generation than generating nonsense. -->
                <contextOverflowPolicy>DISCARD</contextOverflowPolicy>
            </config>
        </component>
        
            <!-- OpenAI client -->
<!--        <component id="llm" class="ai.vespa.llm.clients.OpenAI">-->
<!--            <config name = "ai.vespa.llm.clients.llm-client">-->
<!--                <apiKeySecretName>openai-key</apiKeySecretName>-->
<!--                <model>gpt-4o-mini</model>-->
<!--            </config>-->
<!--        </component>-->
        
        <component id="names_extractor" class="ai.vespa.llm.generation.LanguageModelFieldGenerator">
            <config name="ai.vespa.llm.generation.language-model-field-generator">
                <providerId>llm</providerId>
                <promptTemplateFile>files/names_extractor.txt</promptTemplateFile>
            </config>
        </component>

        <component id="questions_generator" class="ai.vespa.llm.generation.LanguageModelFieldGenerator">
            <config name="ai.vespa.llm.generation.language-model-field-generator">
                <providerId>llm</providerId>
                <promptTemplate>Generate 3 questions relevant for this text: {input}</promptTemplate>
            </config>
        </component>
        
        <document-api/>
        
        <search/>

        <nodes count="1">
            <!-- Local models run much faster with GPU. CPUs with many cores can be used for smaller models 
            but expect many seconds in latency per generate per doc.-->
<!--            <resources vcpu="16.0" memory="32Gb" architecture="x86_64" disk="125Gb">-->
<!--                <gpu count="1" memory="16.0Gb"/>-->
<!--            </resources>-->
            <resources vcpu="8.0" memory="32Gb" architecture="x86_64" storage-type="local" disk="225Gb">
                <gpu count="1" memory="16.0Gb"/>
            </resources>
        </nodes>
    </container>

    <content id="content" version="1.0">
        <redundancy>1</redundancy>
        
        <documents>
            <document mode="index" type="passage"/>
        </documents>
        
        <nodes>
            <node hostalias="node1" distribution-key="0"/>
        </nodes>
    </content>

</services>
