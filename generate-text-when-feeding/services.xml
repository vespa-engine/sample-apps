<?xml version="1.0" encoding="utf-8" ?>
<!-- Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root. -->
<services version="1.0">

    <container id="container" version="1.0">
        <component id="local_llm" class="ai.vespa.llm.clients.LocalLLM">
            <config name="ai.vespa.llm.clients.llm-local-client">

                <!-- File is approx 130Mb" -->
                <model url="https://data.vespa-cloud.com/gguf_models/Llama-160M-Chat-v1.Q6_K.gguf" />

                <contextSize>512</contextSize>
                <parallelRequests>1</parallelRequests>
                <maxQueueSize>0</maxQueueSize>
                <maxTokens>10</maxTokens>
            </config>
        </component>
        
        <component id="local_llm_gen" class="ai.vespa.llm.generation.LanguageModelTextGenerator">
            <config name="ai.vespa.llm.generation.language-model-text-generator">
                <providerId>local_llm</providerId>
                <promptTemplateFile>files/prompt.txt</promptTemplateFile>
            </config>
        </component>

        <document-api/>
        
        <search/>

        <nodes>
            <node hostalias="node1"/>
        </nodes>
    </container>

    <content id="content" version="1.0">
        <redundancy>1</redundancy>
        
        <documents>
            <document mode="index" type="passage"/>
        </documents>
        
        <nodes>
            <node hostalias="node1" distribution-key="0"/>
        </nodes>
    </content>

</services>
