{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning to rank with Transformer models\n",
    "\n",
    "This notebook demonstrates how to train a cross-encoder and a bi-encoder for product ranking. This notebook is part\n",
    "of the [commerce product ranking sample app](https://github.com/vespa-engine/sample-apps/tree/master/commerce-product-ranking). \n",
    "\n",
    "Blog post series:\n",
    "\n",
    "* [Improving Product Search with Learning to Rank - part one](https://blog.vespa.ai/improving-product-search-with-ltr/)\n",
    "* [Improving Product Search with Learning to Rank - part two](https://blog.vespa.ai/improving-product-search-with-ltr-part-two/)\n",
    "\n",
    "This work uses the largest product relevance dataset released by Amazon:\n",
    "\n",
    ">We introduce the “Shopping Queries Data Set”, a large dataset of difficult search queries, released with the aim of fostering research in the area of semantic matching of queries and products. For each query, the dataset provides a list of up to 40 potentially relevant results, together with ESCI relevance judgements (Exact, Substitute, Complement, Irrelevant) indicating the relevance of the product to the query. Each query-product pair is accompanied by additional information. The dataset is multilingual, as it contains queries in English, Japanese, and Spanish.\n",
    "\n",
    "The dataset is found at [amazon-science/esci-data](https://github.com/amazon-science/esci-data). \n",
    "The dataset and is released under the [Apache 2.0 license](https://github.com/amazon-science/esci-data/blob/main/LICENSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vTxXO6iz11QA"
   },
   "outputs": [],
   "source": [
    "!pip3 install --upgrade pandas requests sentence-transformers transformers pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QeYjs8me2eh6"
   },
   "outputs": [],
   "source": [
    "!git lfs clone https://github.com/amazon-science/esci-data.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The field we want to train the two models on and the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39jIlLtdqjF1"
   },
   "outputs": [],
   "source": [
    "document_field=\"product_title\"\n",
    "batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mPtl2Rpf2tUy"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.evaluation import CERerankingEvaluator\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from sentence_transformers import evaluation\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DrCmBTY423QN"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data file pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zTMDayWK29Vl"
   },
   "outputs": [],
   "source": [
    "df_examples = pd.read_parquet('esci-data/shopping_queries_dataset/shopping_queries_dataset_examples.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHnYbG6n4aE4"
   },
   "outputs": [],
   "source": [
    "df_products = pd.read_parquet('esci-data/shopping_queries_dataset/shopping_queries_dataset_products.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h9-aen_RHSmp"
   },
   "outputs": [],
   "source": [
    "df_examples_products = pd.merge(\n",
    "        df_examples,\n",
    "        df_products,\n",
    "        how='left',\n",
    "        left_on=['product_locale', 'product_id'],\n",
    "        right_on=['product_locale', 'product_id']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The esci labels mapping to gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kHYgt-v_HxNi"
   },
   "outputs": [],
   "source": [
    "esci_label2gain = {\n",
    "        'E' : 1,\n",
    "        'S' : 0.1,\n",
    "        'C' : 0.01,\n",
    "        'I' : 0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter on English (US) queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uVd8Gg2zH3d3"
   },
   "outputs": [],
   "source": [
    "df_examples_products = df_examples_products[df_examples_products['small_version'] == 1]\n",
    "df_examples_products = df_examples_products[df_examples_products['split'] == \"train\"]\n",
    "df_examples_products = df_examples_products[df_examples_products['product_locale'] == 'us']\n",
    "df_examples_products['gain'] = df_examples_products['esci_label'].apply(lambda esci_label: esci_label2gain[esci_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QbTIf13fl_pe"
   },
   "source": [
    "Download our own train/dev split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "moFvci3uldkR"
   },
   "outputs": [],
   "source": [
    "train_queries = pd.read_parquet(\"https://data.vespa.oath.cloud/sample-apps-data/train_query_ids.parquet\")['query_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0aq4TwrhI70I"
   },
   "outputs": [],
   "source": [
    "df_examples_products = df_examples_products[['query_id', 'query', 'product_title','product_description', 'product_bullet_point', 'gain']]\n",
    "df_train = df_examples_products[df_examples_products['query_id'].isin(train_queries)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iyl2hLGrs6EP"
   },
   "outputs": [],
   "source": [
    "def replace_none(text):\n",
    "  if text == None:\n",
    "    text = ''\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kyj-XYGbJW3-"
   },
   "outputs": [],
   "source": [
    "train_samples = []\n",
    "for (_, row) in df_train.iterrows():\n",
    "  train_samples.append(InputExample(texts=[row['query'], replace_none(row[document_field])], label=float(row['gain'])))\n",
    "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train cross-encoder \n",
    "Define the model and training parameters. Notice the number of labels is one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PCsi_zz7Kdb5"
   },
   "outputs": [],
   "source": [
    "model_name = 'cross-encoder/ms-marco-MiniLM-L-6-v2'\n",
    "num_epochs = 2\n",
    "num_labels = 1\n",
    "max_length = 96\n",
    "    \n",
    "model = CrossEncoder(\n",
    "  model_name, \n",
    "  num_labels=num_labels, \n",
    "  max_length=max_length, \n",
    "  default_activation_function=torch.nn.Identity(), \n",
    "  device=device\n",
    ")\n",
    "loss_fct=torch.nn.MSELoss()\n",
    "warmup_steps = 10\n",
    "lr = 4e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W_YbUT6nLQbv"
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "  train_dataloader=train_dataloader,\n",
    "  loss_fct=loss_fct,\n",
    "  epochs=num_epochs,\n",
    "  optimizer_params={'lr': lr},\n",
    ")\n",
    "model.save(\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training done - now we upload the model weights to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MsCjw8buRtUR"
   },
   "outputs": [],
   "source": [
    "token='HF_TOKEN' # To upload model to Hugging Face "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_ToaQhsR1P8"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gvd3dZD-SiBC"
   },
   "outputs": [],
   "source": [
    "automodel = AutoModelForSequenceClassification.from_pretrained(\"./model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g3YTbSQUSrtU"
   },
   "outputs": [],
   "source": [
    "autotokenizer = AutoTokenizer.from_pretrained(\"./model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZjF1BqBanRCG"
   },
   "outputs": [],
   "source": [
    "name = document_field + \"_ranker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5l71_N63SwAQ"
   },
   "outputs": [],
   "source": [
    "automodel.push_to_hub(name, use_auth_token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2QIZNz5S-NV"
   },
   "outputs": [],
   "source": [
    "autotokenizer.push_to_hub(name, use_auth_token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_ugeHDCjF1Y"
   },
   "source": [
    "## Train bi-encoder with mean-pooling and Cosine Similarity (angular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJS_dpbgigCy"
   },
   "outputs": [],
   "source": [
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGwxOjziiy5v"
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "  train_objectives=[(train_dataloader, train_loss)],\n",
    "  epochs=num_epochs,\n",
    "  output_path=\"bi-encoder\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXI_lE5_pxle"
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rEsYWzKCp1EB"
   },
   "outputs": [],
   "source": [
    "autmodel = BertModel.from_pretrained(\"./bi-encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lhIwp1FEntZs"
   },
   "outputs": [],
   "source": [
    "name = document_field + \"_encoder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ImvYrM6p6fv"
   },
   "outputs": [],
   "source": [
    "autmodel.push_to_hub(name, use_auth_token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MgZCIKspqIJE"
   },
   "outputs": [],
   "source": [
    "autotokenizer.push_to_hub(name, use_auth_token=token)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
