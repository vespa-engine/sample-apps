{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "excess-listing",
   "metadata": {},
   "source": [
    "# Compare pre-trained CLIP models for text-image retrieval\n",
    "\n",
    "## Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-riverside",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-characterization",
   "metadata": {},
   "source": [
    "## CLIP model\n",
    "\n",
    "There are multiple CLIP model variations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-earth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bab704a",
   "metadata": {},
   "source": [
    "To limit run-time of this notebook, do not use all models - modify the set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcfce42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit number of models to test - here, first two models\n",
    "use_models = clip.available_models()[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-freedom",
   "metadata": {},
   "source": [
    "Each model has an embedding size, needed in the text-image search application schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-magic",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_info = {name: clip.load(name)[0].visual.output_dim for name in use_models}\n",
    "embedding_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-question",
   "metadata": {},
   "source": [
    "## Create and deploy a text-image search app\n",
    "\n",
    "### Create the Vespa application package\n",
    "\n",
    "The function `create_text_image_app` below uses [the Vespa python API](https://pyvespa.readthedocs.io/en/latest/) to create an application package with fields to store each of the different types of image embedding associated with the CLIP models. It also declares the types of the text embeddings that we are going to send along with the query when searching for images, and creates one ranking profile for each (text, image) embedding model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "from embedding import create_text_image_app\n",
    "\n",
    "app_package = create_text_image_app(embedding_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-fence",
   "metadata": {},
   "source": [
    "Inspect the `schema` of the resulting application package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(app_package.schema.schema_to_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-report",
   "metadata": {},
   "source": [
    "### Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-spray",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from vespa.deployment import VespaDocker\n",
    "\n",
    "vespa_docker = VespaDocker()\n",
    "app = vespa_docker.deploy(application_package=app_package)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-supervision",
   "metadata": {},
   "source": [
    "## Compute and feed image embeddings\n",
    "\n",
    "Get a sample data set. See [download_flickr8k.sh](https://github.com/vespa-engine/sample-apps/blob/master/text-image-search/src/sh/download_flickr8k.sh) for how to download images. Set location of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfa9f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env IMG_DIR=./data/Flicker8k_Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bb7023",
   "metadata": {},
   "source": [
    "For each of the CLIP models, compute the image embeddings and send it to the Vespa app:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-capital",
   "metadata": {},
   "outputs": [],
   "source": [
    "from embedding import compute_and_send_image_embeddings\n",
    "\n",
    "compute_and_send_image_embeddings(app=app, batch_size=128, clip_model_names=use_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-chester",
   "metadata": {},
   "source": [
    "## Define QueryModel's to be evaluated\n",
    "\n",
    "Create one `QueryModel` for each of the CLIP models. In order to do that, we need to have a function that takes a query as input and outputs the body function of a Vespa query request - example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-observation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from embedding import create_vespa_query_body_function\n",
    "\n",
    "vespa_query_body_function = create_vespa_query_body_function(\"RN50\")\n",
    "vespa_query_body_function(\"this is a test query\")[\"yql\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-pantyhose",
   "metadata": {},
   "source": [
    "With a method to create Vespa query body functions, we can create `QueryModel`s that will be used to evaluate each search configuration that is to be tested. In this case, each query model will represent a CLIP model text-image representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-steering",
   "metadata": {},
   "outputs": [],
   "source": [
    "from learntorank.query import QueryModel\n",
    "\n",
    "query_models = [QueryModel(\n",
    "    name=model_name, \n",
    "    body_function=create_vespa_query_body_function(model_name)\n",
    ") for model_name in use_models]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-window",
   "metadata": {},
   "source": [
    "A query model contains all the information that is necessary to define how the search app will match and rank documents. Use it to query the application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-relation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from embedding import plot_images\n",
    "from learntorank.query import send_query\n",
    "\n",
    "query_result = send_query(app, query=\"a person surfing\", query_model=query_models[-1], hits = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8012d30",
   "metadata": {},
   "source": [
    "To inspect the results, use `query_result.hits[0]`. Display top two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7890cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "image_file_names = [ hit[\"fields\"][\"image_file_name\"] for hit in query_result.hits[:2] ]\n",
    "\n",
    "for image in image_file_names:\n",
    "    display(Image(filename=os.path.join(os.environ[\"IMG_DIR\"], image)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-drinking",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "Now that there is one QueryModel for each CLIP model available, it is posible to evaluate and compare them. \n",
    "\n",
    "Define search evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-angel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from learntorank.evaluation import MatchRatio, Recall, ReciprocalRank\n",
    "\n",
    "eval_metrics = [\n",
    "    MatchRatio(), # Match ratio is just to show the % of documents that are matched by ANN\n",
    "    Recall(at=100), \n",
    "    ReciprocalRank(at=100)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-needle",
   "metadata": {},
   "source": [
    "Load labeled data. It was assumed that a (caption, image) pair is relevant if all three experts agreed that the caption accurately described the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "labeled_data = read_csv(\"https://data.vespa.oath.cloud/blog/flickr8k/labeled_data.csv\", sep = \"\\t\")\n",
    "labeled_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-graham",
   "metadata": {},
   "source": [
    "Evaluate the application and return per query results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-melbourne",
   "metadata": {},
   "outputs": [],
   "source": [
    "from learntorank.evaluation import evaluate\n",
    "\n",
    "result = evaluate(\n",
    "    app=app,\n",
    "    labeled_data=labeled_data, \n",
    "    eval_metrics=eval_metrics, \n",
    "    query_model=query_models, \n",
    "    id_field=\"image_file_name\",\n",
    "    per_query=True\n",
    ")\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-frame",
   "metadata": {},
   "source": [
    "Visualize RR@100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a35442d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(result.reciprocal_rank_100)\n",
    "plt.ylabel(\"reciprocal_rank_100\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-cement",
   "metadata": {},
   "source": [
    "Compute mean and median across models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-diabetes",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[[\"model\", \"reciprocal_rank_100\"]].groupby(\n",
    "    \"model\"\n",
    ").agg(\n",
    "    Mean=('reciprocal_rank_100', 'mean'), \n",
    "    Median=('reciprocal_rank_100', 'median')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a2bef6",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Stop and remove the Docker container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fa9415",
   "metadata": {},
   "outputs": [],
   "source": [
    "vespa_docker.container.stop()\n",
    "vespa_docker.container.remove()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}