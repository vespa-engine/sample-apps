{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "excess-listing",
   "metadata": {},
   "source": [
    "# Compare pre-trained CLIP models for text-image retrieval\n",
    "> Create, deploy, feed and evaluate the Vespa app using the Vespa python API "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-adelaide",
   "metadata": {},
   "source": [
    "## Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-riverside",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfa9f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env IMG_DIR=."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-characterization",
   "metadata": {},
   "source": [
    "## CLIP model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-transcript",
   "metadata": {},
   "source": [
    "There are multiple CLIP model variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-earth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-freedom",
   "metadata": {},
   "source": [
    "Each model might have a different embedding size. We need this information when creating the schema of a text-image search application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-magic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RN50': 1024,\n",
       " 'RN101': 512,\n",
       " 'RN50x4': 640,\n",
       " 'RN50x16': 768,\n",
       " 'RN50x64': 1024,\n",
       " 'ViT-B/32': 512,\n",
       " 'ViT-B/16': 512,\n",
       " 'ViT-L/14': 768,\n",
       " 'ViT-L/14@336px': 768}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_info = {name: clip.load(name)[0].visual.output_dim for name in clip.available_models()}\n",
    "embedding_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-remainder",
   "metadata": {},
   "source": [
    "## Create and deploy a text-image search app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-territory",
   "metadata": {},
   "source": [
    "### Create the Vespa application package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-question",
   "metadata": {},
   "source": [
    "The function `create_text_image_app` below uses [the Vespa python API](https://pyvespa.readthedocs.io/en/latest/) to create an application package with fields to store each of the six different types of image embedding associated with the CLIP models. It also declares the types of the text embeddings that we are going to send along with the query when searching for images, and creates one ranking profile for each (text, image) embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "from embedding import create_text_image_app\n",
    "\n",
    "app_package = create_text_image_app(embedding_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-fence",
   "metadata": {},
   "source": [
    "We can inspect how the `schema` of the resulting application package looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(app_package.schema.schema_to_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-report",
   "metadata": {},
   "source": [
    "### Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-spray",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from vespa.deployment import VespaDocker\n",
    "\n",
    "vespa_docker = VespaDocker()\n",
    "app = vespa_docker.deploy(application_package=app_package)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-institution",
   "metadata": {},
   "source": [
    "## Compute and feed image embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-supervision",
   "metadata": {},
   "source": [
    "For each of the CLIP models, compute the image embeddings and send it to the Vespa app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-capital",
   "metadata": {},
   "outputs": [],
   "source": [
    "from embedding import compute_and_send_image_embeddings\n",
    "\n",
    "compute_and_send_image_embeddings(app=app, batch_size=128, clip_model_names=clip.available_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-nursery",
   "metadata": {},
   "source": [
    "## Define QueryModel's to be evaluated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-chester",
   "metadata": {},
   "source": [
    "Create one `QueryModel` for each of the CLIP models. In order to do that, we need to have a function that takes a query as input and outputs the body function of a Vespa query request. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-observation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'select * from sources * where ({\"targetNumHits\":100}nearestNeighbor(rn50_image,rn50_text))'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from embedding import create_vespa_query_body_function\n",
    "\n",
    "vespa_query_body_function = create_vespa_query_body_function(\"RN50\")\n",
    "vespa_query_body_function(\"this is a test query\")[\"yql\"] # There are more key, value pairs not shown to unclutter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-pantyhose",
   "metadata": {},
   "source": [
    "With a method to create Vespa query body functions, we can create `QueryModel`s that will be used to evaluate each search configuration that is to be tested. In this case, each query model will represent a CLIP model text-image representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-steering",
   "metadata": {},
   "outputs": [],
   "source": [
    "from learntorank.query import QueryModel\n",
    "\n",
    "query_models = [QueryModel(\n",
    "    name=model_name, \n",
    "    body_function=create_vespa_query_body_function(model_name)\n",
    ") for model_name in clip.available_models()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-window",
   "metadata": {},
   "source": [
    "A query model contains all the information that is necessary to define how the search app will match and rank documents. We can use it to query the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-relation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from embedding import plot_images\n",
    "from learntorank.query import send_query\n",
    "\n",
    "query_result = send_query(app, query=\"a man surfing\", query_model=query_models[3], hits = 4)\n",
    "plot_images(query_result, os.environ[\"IMG_DIR\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-stopping",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-drink",
   "metadata": {},
   "source": [
    "Now that there is one QueryModel for each CLIP model available, it is posible to evaluate and compare them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-drinking",
   "metadata": {},
   "source": [
    "Define search evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-angel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from learntorank.evaluation import MatchRatio, Recall, ReciprocalRank\n",
    "\n",
    "eval_metrics = [\n",
    "    MatchRatio(), # Match ratio is just to show the % of documents that are matched by ANN\n",
    "    Recall(at=100), \n",
    "    ReciprocalRank(at=100)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-needle",
   "metadata": {},
   "source": [
    "Load labeled data. It was assumed that a (caption, image) pair is relevant if all three experts agreed that the caption accurately described the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "labeled_data = read_csv(\"https://data.vespa.oath.cloud/blog/flickr8k/labeled_data.csv\", sep = \"\\t\")\n",
    "labeled_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-graham",
   "metadata": {},
   "source": [
    "Evaluate the application and return per query results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-melbourne",
   "metadata": {},
   "outputs": [],
   "source": [
    "from learntorank.evaluation import evaluate\n",
    "\n",
    "result = evaluate(\n",
    "    app=app,\n",
    "    labeled_data=labeled_data, \n",
    "    eval_metrics=eval_metrics, \n",
    "    query_model=query_models, \n",
    "    id_field=\"image_file_name\",\n",
    "    per_query=True\n",
    ")\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-frame",
   "metadata": {},
   "source": [
    "Visualize RR@100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-western",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.box(result, x=\"model\", y=\"reciprocal_rank_100\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-cement",
   "metadata": {},
   "source": [
    "Compute mean and median across models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-diabetes",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[[\"model\", \"reciprocal_rank_100\"]].groupby(\n",
    "    \"model\"\n",
    ").agg(\n",
    "    Mean=('reciprocal_rank_100', 'mean'), \n",
    "    Median=('reciprocal_rank_100', 'median')\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
