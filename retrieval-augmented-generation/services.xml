<?xml version="1.0" encoding="utf-8" ?>
<!-- Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root. -->
<services version="1.0" xmlns:deploy="vespa" xmlns:preprocess="properties" minimum-required-vespa-version="8.327.49">

  <container id="default" version="1.0">

    <document-api/>

    <!-- Setup the client to OpenAI -->
    <component id="openai" class="ai.vespa.llm.clients.OpenAI" />

    <!-- Setup a local inference on a Mistral 7B model -->
    <!-- Comment out this component to avoid downloading the model file during startup -->
    <component id="mistral" class="ai.vespa.llm.clients.LocalLLM">
      <config name="ai.vespa.llm.clients.llm-local-client">
        <model url="https://data.vespa.oath.cloud/gguf_models/Phi-3-mini-4k-instruct-q4.gguf" />
        <contextSize>4096</contextSize>
        <parallelRequests>1</parallelRequests>
      </config>
    </component>

    <search>

      <chain id="openai" inherits="vespa">
        <searcher id="ai.vespa.search.llm.RAGSearcher">
          <config name="ai.vespa.search.llm.llm-searcher">
            <providerId>openai</providerId>
          </config>
        </searcher>
      </chain>

      <chain id="local" inherits="vespa">
        <searcher id="ai.vespa.search.llm.RAGSearcher">
          <config name="ai.vespa.search.llm.llm-searcher">
            <providerId>mistral</providerId>
          </config>
        </searcher>
      </chain>

    </search>

    <nodes count="1" deploy:environment="dev">
      <resources vcpu="4.0" memory="16Gb" architecture="x86_64" storage-type="local" disk="125Gb">
        <gpu count="1" memory="16.0Gb"/>
      </resources>
    </nodes>

  </container>

  <content id="msmarco" version="1.0">
    <redundancy>1</redundancy>
    <documents>
      <document mode="index" type="passage"/>
    </documents>
    <nodes>
      <node hostalias="node1" distribution-key="0" />
    </nodes>
  </content>

</services>
